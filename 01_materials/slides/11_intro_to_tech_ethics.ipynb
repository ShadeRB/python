{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5fee9e",
   "metadata": {},
   "source": [
    "# Intro to Tech Ethics\n",
    "We will explore tech ethics through real-world case studies.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c69faa",
   "metadata": {
    "id": "63c69faa"
   },
   "source": [
    "# Contents\n",
    "\n",
    "1. Case Study 1: COMPAS & Algorithmic Bias in Criminal Justice\n",
    "2. Case Study 2: Facebook & Cambridge Analytica\n",
    "3. Case Study 3: Apple Card Gender Bias\n",
    "4. Case Study 4: TikTok Algorithm & Mental Health\n",
    "5. Case Study 5: Boeing 737 MAX Software Failure\n",
    "6. Case Study 6: Uber Self-Driving Car Fatality\n",
    "7. Case Study 7: Google Photos Tagging Scandal\n",
    "8. Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7cf83",
   "metadata": {},
   "source": [
    "## Case Study 1: COMPAS & Algorithmic Bias in Criminal Justice\n",
    "\n",
    "**Overview:**\n",
    "In 2016, investigative journalists at *ProPublica* analyzed a tool called COMPAS used in the U.S. criminal justice system to assess the likelihood of defendants reoffending. They found that Black defendants were nearly twice as likely as white defendants to be incorrectly labeled as high risk, while white defendants were often mislabeled as low risk. The tool's predictions were based on opaque algorithms trained on historical data, which itself reflected systemic racial biases.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Opaque decision-making in high-stakes environments\n",
    "- Discrimination due to biased training data\n",
    "- Lack of accountability or recourse for affected individuals\n",
    "\n",
    "**Source:**  \n",
    "[How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ed806",
   "metadata": {},
   "source": [
    "## Case Study 2: Facebook & Cambridge Analytica\n",
    "\n",
    "**Overview:**\n",
    "In 2018, it was found that Cambridge Analytica, a political consulting firm, had used data improperly obtained from Facebook to build psychological profiles and target voters with manipulative ads during the 2016 U.S. presidential election and the Brexit referendum.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Lack of informed user consent\n",
    "- Misuse of personal data for political gain\n",
    "- Violations of platform and privacy norms\n",
    "\n",
    "**Source:**  \n",
    "[Cambridge Analytica and Facebook: The Scandal and the Fallout So Far](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564c26d",
   "metadata": {},
   "source": [
    "## Case Study 3: Apple Card Gender Bias\n",
    "\n",
    "**Overview:**\n",
    "In 2019, both tech experts and regular users reported that the Apple Card, issued by Goldman Sachs, offered significantly different credit limits to men and women, even when they shared finances or had similar credit histories.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Gender discrimination in financial technology\n",
    "- Lack of transparency and explainability in algorithms\n",
    "- Consequences of automated decisions on real lives\n",
    "\n",
    "**Source:**  \n",
    "[Apple's 'sexist' credit card investigated by US regulator\n",
    "](https://www.bbc.com/news/business-50365609)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4aa32e",
   "metadata": {},
   "source": [
    "## Case Study 4: TikTok Algorithm & Mental Health\n",
    "\n",
    "**Overview:**\n",
    "Studies have shown that TikTok’s recommendation system led new users, especially teenagers, into harmful content spirals. For example, users who briefly interacted with videos about sadness were quickly shown an overwhelming number of videos on self-harm and depression.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Exploitation of user behavior to drive engagement\n",
    "- Psychological harm to vulnerable populations\n",
    "- Lack of oversight or content safeguards\n",
    "\n",
    "**Source:**  \n",
    "[Global: TikTok’s ‘For You’ feed risks pushing children and young people towards harmful mental health content](https://www.amnesty.org/en/latest/news/2023/11/tiktok-risks-pushing-children-towards-harmful-content/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb68f81",
   "metadata": {},
   "source": [
    "## Case Study 5: Boeing 737 MAX Software Failure\n",
    "\n",
    "**Overview:**\n",
    "Two Boeing 737 MAX aircraft crashed in 2018 and 2019, killing 346 people. The cause was a software system called MCAS, which automatically pushed the plane's nose down based on data from a single faulty sensor. Boeing failed to disclose the system’s function to pilots, and pilot training did not include information on how to override it. Software intended to improve flight safety ended up overriding human control fatally.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Concealment of critical system behavior\n",
    "- Insufficient pilot training and documentation\n",
    "- Over-reliance on automation in safety-critical systems\n",
    "\n",
    "**Source:**  \n",
    "[NYT Analysis](https://www.nytimes.com/2019/11/03/business/boeing-737-max-crashes.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78caea5",
   "metadata": {},
   "source": [
    "## Case Study 6: Uber Self-Driving Car Fatality\n",
    "\n",
    "**Overview:**\n",
    "In 2018, a self-driving Uber vehicle killed a pedestrian in Tempe, Arizona- the first known fatality caused by an autonomous car. The system failed to classify the pedestrian as a threat and made no attempt to stop. The backup human driver, intended to monitor the system, failed to intervene in time. It later emerged that the software was deliberately programmed to ignore some objects considered \"false positives,\" reducing safety.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Responsibility in autonomous systems\n",
    "- Weak safety protocols in experimental tech\n",
    "- Human-computer interaction and over-trust in automation\n",
    "\n",
    "**Source:**  \n",
    "[Uber self-driving car test driver pleads guilty to endangerment in pedestrian death case](https://edition.cnn.com/2023/07/29/business/uber-self-driving-car-death-guilty/index.html)\n",
    "\n",
    "[Uber’s self-driving car saw the pedestrian but didn’t swerve – report](https://www.theguardian.com/technology/2018/may/08/ubers-self-driving-car-saw-the-pedestrian-but-didnt-swerve-report#:~:text=She%20later%20died%20from%20her,a%20report%20from%20the%20Information.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997493d",
   "metadata": {},
   "source": [
    "## Case Study 7: Google Photos Tagging Scandal\n",
    "\n",
    "**Overview:**\n",
    "In 2015, Google Photos' image recognition system mistakenly labeled Black individuals as \"gorillas.\" The error was a result of biased training data and lack of testing across diverse datasets. Google’s response was to remove the label entirely rather than addressing the underlying model flaws. This raised major concerns about racial bias in AI and the ethics of reactive vs proactive fixes.\n",
    "\n",
    "**Ethical Concerns:**\n",
    "- Racial bias in machine learning training data\n",
    "- Ethical failure to address root causes\n",
    "- The reputational impact of deploying flawed AI\n",
    "\n",
    "**Source:**  \n",
    "[Google's solution to accidental algorithmic racism: ban gorillas](https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa2286",
   "metadata": {
    "id": "c7fa2286"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Rachel Thomas\n",
    "\n",
    "[Rachel Thomas](https://rachel.fast.ai/about.html) is a co-founder of __fast.ai__ - a great resource to get introduced to tech ethics.\n",
    "<br>\n",
    "\n",
    "Dr Thomas compiled short videos as a quick accessible source of artificial intelligence ethics. You can watch and discuss one of the videos on Slack, called [What are Ethics and Why do they Matter? Machine Learning edition](https://www.youtube.com/watch?v=F0cxzESR7ec&list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR).\n",
    "\n",
    "Data analysis and machine learning solutions must be built with human rights and freedoms in mind. As Rachel Thomas writes in Avoiding Data Disasters, “The people most impacted by errors must be included as partners in the design of the project.”\n",
    "\n",
    "### Casey Fiesler\n",
    "\n",
    "Professor **Casey Fiesler** of the University of Colorado Boulder analyzed over 250 tech ethics syllabi, studying the topics covered and the teaching goals. This work led to the paper, [*What Do We Teach When We Teach Tech Ethics? A Syllabi Analysis*](https://cmci.colorado.edu/~cafi5706/SIGCSE2020_EthicsSyllabi.pdf), co-authored with Natalie Garrett and Nathan Beard.\n",
    "\n",
    "Fiesler et al. show that tech ethics covers many topics, and this module cannot explore all of them in detail. Instead, the goal of this module is to help us assess technical work, identify ethical issues, and critique existing systems.\n",
    "\n",
    "### Vicky Boykis\n",
    "\n",
    "**Vicky Boykis** is a machine learning engineer at Tumblr and a writer on tech ethics. On her blog and in newsletters, she explores the impact of technology on society. \n",
    "\n",
    "In *[Neural Nets Are Just People All the Way Down](https://vicki.substack.com/p/neural-nets-are-just-people-all-the)*, Boykis traces image recognition systems back to their human origins, showing that even modern systems are built on datasets tagged by people, like those from Mechanical Turk in 2007.\n",
    "\n",
    "Both Rachel Thomas and Boykis argue that tech jobs and methods should be “uncool” to avoid rushing into projects driven by shiny new technologies. Instead, tech ethics should start with collaboration among stakeholders, industry experts, and technical teams.\n",
    "\n",
    "Boykis also highlights the risks of Big Data, stating: “If the first ten years of data science were about collecting everything, the second ten will be about being selective in data collection.”\n",
    "\n",
    "Read more in *[Logs Were Our Lifeblood. Now They're Our Liability.](https://vicki.substack.com/p/logs-were-our-lifeblood-now-theyre)*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "7-Ethics_slides.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
